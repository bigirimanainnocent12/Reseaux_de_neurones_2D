{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPy+MReZek9dIOIa1nFceB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bigirimanainnocent12/Reseaux_de_neurones_2D/blob/main/Untitled30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objectif du Projet**\n",
        "\n",
        "Le dataset SPEECHCOMMANDS contient des fichiers audio associés à des commandes\n",
        "vocales, ainsi que les transcriptions correspondantes. Ce projet s’appuie sur un tutoriel officiel disponible ici: speech_command_classification_with_torchaudio_tutorial.\n",
        "Dans ce tutoriel, un réseau de neurones convolutionnel 1D est utilisé pour associer les données audio aux transcriptions. Cependant, il est courant d’effectuer la classification audio dans le domaine temps-fréquence, à partir d’un spectrogramme. L’objectif du projet est donc de concevoir un réseau de neurones convolutionnel 2D capable de travailler sur les spectrogrammes, tout en cherchant à optimiser ses performances. Pour ce faire, vous utiliserez la librairie torchaudio et la transformation MelSpectrogram pour générer les spectrogrammes à partir des fichiers audio."
      ],
      "metadata": {
        "id": "sdLWtVCz6vvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importation des librairies necessaires**"
      ],
      "metadata": {
        "id": "RAoGcsmo7Pet"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bToFlAhU6kn5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chargement des ensembles d'entraînement, validation et test**"
      ],
      "metadata": {
        "id": "OuyO8K8D7bWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(\"./\", download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as fileobj:\n",
        "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]"
      ],
      "metadata": {
        "id": "0ZQkT2wM7dGD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prétraitement : Transformation en spectrogrammes**"
      ],
      "metadata": {
        "id": "TuT2UzQ97n5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioTransform:\n",
        "    def __init__(self):\n",
        "        self.mel_spectrogram = MelSpectrogram(\n",
        "            sample_rate=16000, n_fft=1024, hop_length=512, n_mels=64\n",
        "        )\n",
        "        self.db_transform = AmplitudeToDB(top_db=80)\n",
        "\n",
        "    def __call__(self, waveform):\n",
        "        mel = self.mel_spectrogram(waveform)\n",
        "        db_mel = self.db_transform(mel)\n",
        "        return db_mel"
      ],
      "metadata": {
        "id": "-yV6TBCV7ik-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Padding des spectrogrammes pour les rendre de taille fixe**"
      ],
      "metadata": {
        "id": "XPsr7uhy7wNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_spectrogram(spectrogram, max_length=128):\n",
        "    if spectrogram.size(2) < max_length:\n",
        "        pad_size = max_length - spectrogram.size(2)\n",
        "        spectrogram = torch.nn.functional.pad(spectrogram, (0, pad_size))\n",
        "    elif spectrogram.size(2) > max_length:\n",
        "        spectrogram = spectrogram[:, :, :max_length]\n",
        "    return spectrogram"
      ],
      "metadata": {
        "id": "Armh1MJK70PL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fonction de gestion des batchs**"
      ],
      "metadata": {
        "id": "EohuFuAu719q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    waveforms, labels = zip(*[(transform(waveform), label_to_index[label]) for waveform, _, label, _, _ in batch])\n",
        "    waveforms = [pad_spectrogram(waveform) for waveform in waveforms]\n",
        "    waveforms = torch.stack(waveforms)  # [batch_size, 1, n_mels, max_length]\n",
        "    labels = torch.tensor(labels)\n",
        "    return waveforms, labels"
      ],
      "metadata": {
        "id": "QTvLbaS6750a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modèle CNN 2D**"
      ],
      "metadata": {
        "id": "gZVkxcG28BBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet2D(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNet2D, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # [batch, 32, 64, 128]\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) # [batch, 64, 64, 128]\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Réduction de moitié dans les dimensions spatiales\n",
        "\n",
        "        # Calcul de la taille d'entrée pour fc1\n",
        "        # Après conv1 -> pool -> conv2 -> pool\n",
        "        # Taille finale : [batch_size, 64, 16, 32] pour une entrée [batch_size, 1, 64, 128]\n",
        "        self.flatten_size = 64 * 16 * 32\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)  # [batch, 32, 32, 64]\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)  # [batch, 64, 16, 32]\n",
        "        x = x.view(x.size(0), -1)  # Flatten : [batch, 64 * 16 * 32]\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ny0krGH68E3N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chargement des données**"
      ],
      "metadata": {
        "id": "E4hcW35H8GGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = SubsetSC(\"training\")\n",
        "test_set = SubsetSC(\"testing\")\n",
        "\n",
        "transform = AudioTransform()\n",
        "labels = sorted(list(set(label for _, _, label, _, _ in train_set)))\n",
        "label_to_index = {label: i for i, label in enumerate(labels)}\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "_ppRcxqE8K9C",
        "outputId": "ee9b0905-cdb8-475a-bf97-664f466f6eee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [00:28<00:00, 85.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialisation du modèle, de la perte et de l'optimiseur**"
      ],
      "metadata": {
        "id": "iz93HtWN8UZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(labels)\n",
        "model = ConvNet2D(num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "w2JAYtOQ8bTw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Variables pour suivre les performances**"
      ],
      "metadata": {
        "id": "nCIV1kLe8fSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accuracies = []"
      ],
      "metadata": {
        "id": "p6SkWpSd8lSZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Boucle d'entraînement et Évaluation sur le jeu de test**"
      ],
      "metadata": {
        "id": "liuonMTC8r6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):  # 3 époques, ajustez selon vos besoins\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for spectrograms, targets in train_loader:\n",
        "        spectrograms, targets = spectrograms.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {train_losses[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "oAK1f3ADQt3F",
        "outputId": "3d27bc31-00b9-48ef-f65c-13e3e237f4ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7332\n",
            "Epoch 2, Loss: 0.5609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Évaluation sur le jeu de test**"
      ],
      "metadata": {
        "id": "X1UgTKvpjP9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for spectrograms, targets in test_loader:\n",
        "        spectrograms, targets = spectrograms.to(device), targets.to(device)\n",
        "        outputs = model(spectrograms)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "PimoABmXQt0O",
        "outputId": "01848959-596c-46ec-d5f2-5bca3ed0bbe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 79.32%\n"
          ]
        }
      ]
    }
  ]
}